{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b98e57c9-1c1b-4090-bcbe-adecb6651854",
   "metadata": {},
   "source": [
    "# Object detection\n",
    "\n",
    "Object detection is a more challenging task than simple image classification. It can find named objects within a given image, draw a bounding box around it and even produce multiple annotations if multiple classes exist within an image.\n",
    "\n",
    "This additional complication means that this process is typically slower than simple image classification. As with classification, we first need to load in the relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598d8e7-2c19-40fb-851c-2daa586ab0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all required packages for this example.\n",
    "from PIL import Image  ## This library manages reading images from file.\n",
    "from transformers import pipeline  ## The library manages many pretrained models.\n",
    "import glob  ## This library efficiently works with multiple files in a directory.\n",
    "import matplotlib.pyplot as plt  ## This package is useful for displaying the images.\n",
    "import matplotlib.patches as patches\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcecf26-2313-4a26-8afe-d74fb5e1a1f7",
   "metadata": {},
   "source": [
    "Once again, we need to prepare out dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcdac2d-5b8a-4dc7-b200-78d77bb39071",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'example_data/coco/*'  ## Note * is a wildcard character, meaning match everything.\n",
    "images = sorted(glob.glob(folder))\n",
    "images = [Image.open(i) for i in images[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b26468-3a1a-4cee-a055-84cc8d68701f",
   "metadata": {},
   "source": [
    "When we load our model, we can pass one of a few model options. This time we're going to use `OWL-ViT`, a model developed by Google specifically for zero-shot object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bd281d-5dc3-462e-9cd6-e22ea3997bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"google/owlv2-base-patch16-ensemble\"\n",
    "detector = pipeline(model=model, task=\"zero-shot-object-detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f15ff5-0988-46ef-8e16-c2bf8b29dfd6",
   "metadata": {},
   "source": [
    "Now the model is ready to be applied to images. We define the classes we are looking for in exactly the same way as when we were classifying the image as a whole, with one exception. The object detection task takes a new, optional parameter: `threshold`. This value sets a minimum model score required to recognise an object. This is necessary for object detection, as the model will return all matched objects regardless of the likelihood of a match. Some tuning is required to determine an effective threshold based on the candidate labels and dataset. Here, we found `0.3` to be appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f17f0-a230-4947-b525-f2f99cff667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['elephant','man','woman']\n",
    "detections = [detector(i,candidate_labels = labels,threshold=0.3) for i in images]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a11d8b2-6b58-4e5b-8b23-7aaa36f9bfc2",
   "metadata": {},
   "source": [
    "Once again, we can view the objects detected in a given image. Here we plot the image, and overlay the bounding boxes for each object found in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9f9c9c-fe57-4ac9-8e48-ee44624e6676",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(10):\n",
    "\n",
    "    plt.imshow(images[index])\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    if detections[index]:\n",
    "        for result in detections[index]:\n",
    "            box = result[\"box\"]\n",
    "            label = f\"{result['label']} ({result['score']:.2f})\"\n",
    "            \n",
    "            # Draw bounding box\n",
    "            rect = patches.Rectangle(\n",
    "                (box[\"xmin\"], box[\"ymin\"]),\n",
    "                box[\"xmax\"] - box[\"xmin\"],\n",
    "                box[\"ymax\"] - box[\"ymin\"],\n",
    "                linewidth=2,\n",
    "                edgecolor=\"lime\",\n",
    "                facecolor=\"none\"\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Draw label\n",
    "            ax.text(\n",
    "                box[\"xmin\"],\n",
    "                box[\"ymin\"] - 5,\n",
    "                label,\n",
    "                color=\"black\",\n",
    "                fontsize=10,\n",
    "                backgroundcolor=\"lime\"\n",
    "            )\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c9168e-e750-42e0-8afd-13ea60dfa04a",
   "metadata": {},
   "source": [
    "As with the classification task, there is an element of experimentation required to determine the appropriate codebook and threshold for a given application of zero-shot object detection. You may find that the model is not suitable for your dataset - you can always look for other hosted on [HuggingFace](https://huggingface.co/models?pipeline_tag=zero-shot-object-detection&sort=trending), most of which should work by replacing the model name when making the pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
